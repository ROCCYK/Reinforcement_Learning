<!DOCTYPE html>
<html><head></head><body style="color: rgb(32, 33, 34); font-family: undefined;"><h4 data-end="253" data-start="206">Bellman Equation and Value Functions Recap</h4>
<p data-end="314" data-start="255">In reinforcement learning, we have two key value functions:</p>
<ol data-end="918" data-start="316">
<li data-end="602" data-start="316">
<p data-end="445" data-start="319"><strong data-end="347" data-start="319">State Value Function (V)</strong>: Represents the expected return starting from a given state <math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>s</mi></mrow><annotation encoding="application/x-tex" data-d2l-attr-encoding="application/x-tex">s</annotation></semantics></math><span aria-hidden="true">s</span> and following a policy <math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>π</mi></mrow><annotation encoding="application/x-tex" data-d2l-attr-encoding="application/x-tex">\pi</annotation></semantics></math><span aria-hidden="true">π</span>.</p>
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>V</mi><mi>π</mi></msup><mo stretchy="false">(</mo><mi>s</mi><mo stretchy="false">)</mo><mo>=</mo><mi mathvariant="double-struck">E</mi><mo stretchy="false">[</mo><msub><mi>R</mi><mi>t</mi></msub><mi mathvariant="normal">∣</mi><msub><mi>S</mi><mi>t</mi></msub><mo>=</mo><mi>s</mi><mo separator="true">,</mo><mi>π</mi><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex" data-d2l-attr-encoding="application/x-tex">V^{\pi}(s) = \mathbb{E}[R_t | S_t = s, \pi]</annotation></semantics></math><span aria-hidden="true">Vπ(s)=E[Rt​∣St​=s,π]</span>
<ul data-end="602" data-start="508">
<li data-end="602" data-start="508">It calculates the <strong data-end="558" data-start="528">expected cumulative reward</strong> from a given state under a specific policy.</li>
</ul>
</li>
<li data-end="918" data-start="604">
<p data-end="757" data-start="607"><strong data-end="642" data-start="607">State-Action Value Function (Q)</strong>: Represents the expected return after taking a specific action <math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>a</mi></mrow><annotation encoding="application/x-tex" data-d2l-attr-encoding="application/x-tex">a</annotation></semantics></math><span aria-hidden="true">a</span> from state <math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>s</mi></mrow><annotation encoding="application/x-tex" data-d2l-attr-encoding="application/x-tex">s</annotation></semantics></math><span aria-hidden="true">s</span>, following a policy <math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>π</mi></mrow><annotation encoding="application/x-tex" data-d2l-attr-encoding="application/x-tex">\pi</annotation></semantics></math><span aria-hidden="true">π</span>.</p>
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>Q</mi><mi>π</mi></msup><mo stretchy="false">(</mo><mi>s</mi><mo separator="true">,</mo><mi>a</mi><mo stretchy="false">)</mo><mo>=</mo><mi mathvariant="double-struck">E</mi><mo stretchy="false">[</mo><msub><mi>R</mi><mi>t</mi></msub><mi mathvariant="normal">∣</mi><msub><mi>S</mi><mi>t</mi></msub><mo>=</mo><mi>s</mi><mo separator="true">,</mo><msub><mi>A</mi><mi>t</mi></msub><mo>=</mo><mi>a</mi><mo separator="true">,</mo><mi>π</mi><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex" data-d2l-attr-encoding="application/x-tex">Q^{\pi}(s, a) = \mathbb{E}[R_t | S_t = s, A_t = a, \pi]</annotation></semantics></math><span aria-hidden="true">Qπ(s,a)=E[Rt​∣St​=s,At​=a,π]</span>
<ul data-end="918" data-start="832">
<li data-end="918" data-start="832">It tells us how good a specific action is in a given state under the current policy.</li>
</ul>
</li>
</ol>
<hr data-end="923" data-start="920">
<h4 data-end="969" data-start="925">Bellman Equation: The Recursive Formula</h4>
<p data-end="1198" data-start="971">The <strong data-end="995" data-start="975">Bellman equation</strong> allows us to recursively calculate these values, enabling us to store and reuse calculated values rather than recalculating them each time. This makes the process efficient and computationally feasible.</p>
<p data-end="1268" data-start="1200">The equation for the <strong data-end="1249" data-start="1221">State Value Function (V)</strong> can be written as:</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>V</mi><mi>π</mi></msup><mo stretchy="false">(</mo><mi>s</mi><mo stretchy="false">)</mo><mo>=</mo><munder><mo>∑</mo><mi>a</mi></munder><mi>π</mi><mo stretchy="false">(</mo><mi>a</mi><mi mathvariant="normal">∣</mi><mi>s</mi><mo stretchy="false">)</mo><munder><mo>∑</mo><msup><mi>s</mi><mo rspace="0em" lspace="0em" mathvariant="normal">′</mo></msup></munder><mi>P</mi><mo stretchy="false">(</mo><msup><mi>s</mi><mo rspace="0em" lspace="0em" mathvariant="normal">′</mo></msup><mi mathvariant="normal">∣</mi><mi>s</mi><mo separator="true">,</mo><mi>a</mi><mo stretchy="false">)</mo><mo stretchy="false">[</mo><mi>R</mi><mo stretchy="false">(</mo><mi>s</mi><mo separator="true">,</mo><mi>a</mi><mo separator="true">,</mo><msup><mi>s</mi><mo rspace="0em" lspace="0em" mathvariant="normal">′</mo></msup><mo stretchy="false">)</mo><mo>+</mo><mi>γ</mi><msup><mi>V</mi><mi>π</mi></msup><mo stretchy="false">(</mo><msup><mi>s</mi><mo rspace="0em" lspace="0em" mathvariant="normal">′</mo></msup><mo stretchy="false">)</mo><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex" data-d2l-attr-encoding="application/x-tex">V^{\pi}(s) = \sum_{a} \pi(a|s) \sum_{s'} P(s'|s, a) [R(s, a, s') + \gamma V^{\pi}(s')]</annotation></semantics></math><span aria-hidden="true"></span></p>
<p data-end="1407" data-start="1363">For the <strong data-end="1406" data-start="1371">State-Action Value Function (Q)</strong>:</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>Q</mi><mi>π</mi></msup><mo stretchy="false">(</mo><mi>s</mi><mo separator="true">,</mo><mi>a</mi><mo stretchy="false">)</mo><mo>=</mo><munder><mo>∑</mo><msup><mi>s</mi><mo rspace="0em" lspace="0em" mathvariant="normal">′</mo></msup></munder><mi>P</mi><mo stretchy="false">(</mo><msup><mi>s</mi><mo rspace="0em" lspace="0em" mathvariant="normal">′</mo></msup><mi mathvariant="normal">∣</mi><mi>s</mi><mo separator="true">,</mo><mi>a</mi><mo stretchy="false">)</mo><mo stretchy="false">[</mo><mi>R</mi><mo stretchy="false">(</mo><mi>s</mi><mo separator="true">,</mo><mi>a</mi><mo separator="true">,</mo><msup><mi>s</mi><mo rspace="0em" lspace="0em" mathvariant="normal">′</mo></msup><mo stretchy="false">)</mo><mo>+</mo><mi>γ</mi><munder><mo>∑</mo><msup><mi>a</mi><mo rspace="0em" lspace="0em" mathvariant="normal">′</mo></msup></munder><mi>π</mi><mo stretchy="false">(</mo><msup><mi>a</mi><mo rspace="0em" lspace="0em" mathvariant="normal">′</mo></msup><mi mathvariant="normal">∣</mi><msup><mi>s</mi><mo rspace="0em" lspace="0em" mathvariant="normal">′</mo></msup><mo stretchy="false">)</mo><msup><mi>Q</mi><mi>π</mi></msup><mo stretchy="false">(</mo><msup><mi>s</mi><mo rspace="0em" lspace="0em" mathvariant="normal">′</mo></msup><mo separator="true">,</mo><msup><mi>a</mi><mo rspace="0em" lspace="0em" mathvariant="normal">′</mo></msup><mo stretchy="false">)</mo><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex" data-d2l-attr-encoding="application/x-tex">Q^{\pi}(s, a) = \sum_{s'} P(s'|s, a) [R(s, a, s') + \gamma \sum_{a'} \pi(a'|s') Q^{\pi}(s', a')]</annotation></semantics></math><span aria-hidden="true"></span></p>
<p data-end="1778" data-start="1512">These equations help us <strong data-end="1573" data-start="1536">store and reuse calculated values</strong>, which means we don’t have to compute them from scratch every time. This way, we can <strong data-end="1686" data-start="1659">find the optimal policy</strong> efficiently. The <strong data-end="1722" data-start="1704">optimal policy</strong> is the one that yields the highest value at every step.</p></body></html>
{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d781446",
   "metadata": {},
   "source": [
    "# **Step-by-Step Tutorial: Solving CartPole using Reinforcement Learning**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fc55f44",
   "metadata": {},
   "source": [
    "### **Step 1: Install Necessary Libraries**\n",
    "# Install the Gymnasium library for the CartPole environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40e89db8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gymnasium in c:\\users\\piaka\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (1.0.0)\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\users\\piaka\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from gymnasium) (1.26.4)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\piaka\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from gymnasium) (3.1.1)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in c:\\users\\piaka\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from gymnasium) (4.12.2)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in c:\\users\\piaka\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from gymnasium) (0.0.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install gymnasium"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e9f896",
   "metadata": {},
   "source": [
    "### **Step 2: Import Required Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d78bce57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eeb0b5c",
   "metadata": {},
   "source": [
    "### **Step 3: Initialize the CartPole Environment**\n",
    "# Create the CartPole environment.\n",
    "# Reset the environment to start with an initial observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fcc7dbe6-eac2-4009-bc35-74edfdc13d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2319e5bc-fb43-4a1d-910e-40b2a9c5be0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Observation: [-0.0095345  -0.04972725  0.03152118  0.00246065]\n"
     ]
    }
   ],
   "source": [
    "observation, info = env.reset()\n",
    "print(\"Initial Observation:\", observation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63df92a7-254b-4633-8d8d-a18a5efdcfd6",
   "metadata": {},
   "source": [
    "\"The observation is four floating point numbers: x coordinate of stick's center of mass, its speed, its angle to the platform, its angular speed\n",
    "Remember however our goal is to balance the pole based on rewards only -- not on physics!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d436800",
   "metadata": {},
   "source": [
    "### **Step 4: Understand Action and Observation Space**\n",
    "# Print action and observation spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c96546c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action Space: Discrete(2)\n"
     ]
    }
   ],
   "source": [
    "print(\"Action Space:\", env.action_space)  # Discrete(2): 0 = Push cart left, 1 = Push cart right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "acbd03e2-2499-47b0-94ad-701ce696dae0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation Space: Box([-4.8               -inf -0.41887903        -inf], [4.8               inf 0.41887903        inf], (4,), float32)\n"
     ]
    }
   ],
   "source": [
    "print(\"Observation Space:\", env.observation_space)  # Box(4,): VECTOR of size 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d59c49-bb36-47b9-9876-7ac7e8c80058",
   "metadata": {},
   "source": [
    "# Understanding the Observation Space in Reinforcement Learning\n",
    "\n",
    "In **Reinforcement Learning (RL)**, the **observation space** defines the possible states an agent can observe. Below is an example of an observation space from **OpenAI Gym**, commonly found in environments like **CartPole**:\n",
    "\n",
    "\n",
    "## **Breaking it Down**\n",
    "- **Box**: Represents a continuous space (real-valued numbers).\n",
    "- **Lower bounds**: `[-4.8, -inf, -0.41887903, -inf]` - The minimum values for each of the four state variables.\n",
    "- **Upper bounds**: `[4.8, inf, 0.41887903, inf]` - The maximum values for each state variable.\n",
    "- **(4,)**: The observation space consists of **4 continuous state variables**.\n",
    "- **float32**: The datatype of the values in this space.\n",
    "\n",
    "---\n",
    "\n",
    "## **Key Insights**\n",
    "- The cart can move between **-4.8 and 4.8 meters** before the episode ends.\n",
    "- The pole angle must stay within **± 0.41887903 radians (±24 degrees)** to avoid failure.\n",
    "- Velocity variables (`cart velocity` and `pole angular velocity`) are unbounded (`-inf` to `inf`), meaning they can take any real value.\n",
    "\n",
    "---\n",
    "\n",
    "## **Usage in Reinforcement Learning**\n",
    "This observation space defines the **input state** that the agent observes in the environment. The agent uses this information to decide which **action** to take, aiming to balance the pole upright.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "49f210c5-1623-4be2-97f8-2ebff6d3dec2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-0.01052904, -0.24528675,  0.03157039,  0.30491987], dtype=float32),\n",
       " 1.0,\n",
       " False,\n",
       " False,\n",
       " {})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#lets move to left\n",
    "env.step(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54928e7c-c2c2-47be-b5a0-c8e78d8b390c",
   "metadata": {},
   "source": [
    "# Understanding the Step Output Tuple in OpenAI Gym\n",
    "\n",
    "When calling `env.step(action)` in an **OpenAI Gym** environment, the function returns a tuple with five elements:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9dbb7ed1-b3f8-4363-92eb-48d34f676268",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Next State: [-0.01543478 -0.05062859  0.03766878  0.02235829]\n",
      "Reward: 1.0\n",
      "Done: False\n",
      "Truncated: False\n",
      "Info: {}\n"
     ]
    }
   ],
   "source": [
    "# Take an action (e.g., action = 1)\n",
    "action = 1  # Move right\n",
    "next_state, reward, done, truncated, info = env.step(action)\n",
    "\n",
    "print(\"Next State:\", next_state)\n",
    "print(\"Reward:\", reward)\n",
    "print(\"Done:\", done)\n",
    "print(\"Truncated:\", truncated)\n",
    "print(\"Info:\", info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "edca124c-3e05-4122-8ed3-a07510b9be82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "#call random action using sample()\n",
    "print(env.action_space.sample())\n",
    "print(env.action_space.sample())\n",
    "print(env.action_space.sample())\n",
    "print(env.action_space.sample())\n",
    "print(env.action_space.sample())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f6e78d07-00a2-45b9-94e3-a7b528b716c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.28419644 -0.88558966  0.13893068 -0.5007224 ]\n",
      "[-2.8329377  -1.4832586   0.06538605  0.75585717]\n",
      "[-4.229765    1.6419837   0.14979577 -1.3968955 ]\n",
      "[-1.8428878e+00 -3.2399336e-01  7.0862332e-04  1.5177377e-01]\n"
     ]
    }
   ],
   "source": [
    "#call random obervation states using sample()\n",
    "print(env.observation_space.sample())\n",
    "print(env.observation_space.sample())\n",
    "print(env.observation_space.sample())\n",
    "print(env.observation_space.sample())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be4ed304",
   "metadata": {},
   "source": [
    "### **Step 5:  Run the environment for N steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6195d19c-aba2-4963-ba06-d01251b096d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of episodes\n",
    "num_episodes = 5  # Adjust as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "33e79979-0197-4e08-954d-23bee162a42d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 1 starting...\n",
      "Episode 1 ended after 20 steps with total reward: 20.0\n",
      "\n",
      "Episode 2 starting...\n",
      "Episode 2 ended after 11 steps with total reward: 11.0\n",
      "\n",
      "Episode 3 starting...\n",
      "Episode 3 ended after 34 steps with total reward: 34.0\n",
      "\n",
      "Episode 4 starting...\n",
      "Episode 4 ended after 12 steps with total reward: 12.0\n",
      "\n",
      "Episode 5 starting...\n",
      "Episode 5 ended after 14 steps with total reward: 14.0\n"
     ]
    }
   ],
   "source": [
    "# Create the environment\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "num_episodes = 5  # Define the number of episodes\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    state, info = env.reset(seed=episode)  # Reset environment at the start of each episode\n",
    "    terminated = False\n",
    "    total_reward = 0\n",
    "    step_count = 0\n",
    "\n",
    "    print(f\"\\nEpisode {episode + 1} starting...\")\n",
    "\n",
    "    while not terminated:\n",
    "        #render(env)\n",
    "        action = env.action_space.sample()  # Random action (0 or 1)\n",
    "        state, reward, terminated, truncated, info = env.step(action)\n",
    "        total_reward += reward\n",
    "        step_count += 1\n",
    "\n",
    "    print(f\"Episode {episode + 1} ended after {step_count} steps with total reward: {total_reward}\")\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
